{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _This code runs in the notebook itself_\n",
    "\n",
    "# Fine-Tuning a RoBERTa Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install easydict==1.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['SM_HOSTS']='localhost'\n",
    "os.environ['SM_CURRENT_HOST']='localhost'\n",
    "os.environ['SM_MODEL_DIR']='./model'\n",
    "os.environ['SM_CHANNEL_TRAIN']='./data/sentiment-train'\n",
    "os.environ['SM_CHANNEL_VALIDATION']='./data/sentiment-validation'\n",
    "os.environ['SM_CHANNEL_TEST']='./data/sentiment-test'\n",
    "os.environ['SM_OUTPUT_DIR']='./output'\n",
    "os.environ['SM_NUM_GPUS']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args=easydict.EasyDict({\n",
    "    'train_batch_size': 8,\n",
    "    'train_steps_per_epoch': 1,\n",
    "    'validation_batch_size': 8,\n",
    "    'test_batch_size': 8,\n",
    "    'epochs': 1,\n",
    "    'lr': 2e-5,\n",
    "    'momentum': 0.5,\n",
    "    'seed': 42,\n",
    "    'log_interval': 10,\n",
    "    'backend': 'gloo',\n",
    "    'max_seq_len': 64,\n",
    "    'model_name': 'roberta-base',\n",
    "    'enable_sagemaker_debugger': True,\n",
    "    'run_validation' : False,\n",
    "    'run_test': False,\n",
    "    'run_sample_predictions': False,\n",
    "    'hosts': 'localhost',\n",
    "    'current_host': 'localhost', \n",
    "    'model_dir': './model',\n",
    "    'train_data': './data/sentiment-train',\n",
    "    'validation_data': './data/sentiment-validation',\n",
    "    'test_data': './data/sentiment-test',\n",
    "    'output_dir': './output',\n",
    "    'num_gpus': 0,\n",
    "    'save-frequency': 10,\n",
    "    'smdebug_path': '/opt/ml/output/tensors',\n",
    "    'hook-type': 'saveall'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaModel, RobertaTokenizer, RobertaConfig\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import RobertaForSequenceClassification\n",
    "\n",
    "import smdebug.pytorch as smd\n",
    "from smdebug.pytorch import Hook, SaveConfig\n",
    "from smdebug import modes\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "# Has to be called 'model.pth'\n",
    "MODEL_NAME = 'model.pth'\n",
    "PRE_TRAINED_MODEL_NAME = 'roberta-base'\n",
    "\n",
    "DATA_COLUMN = 'review_body'\n",
    "LABEL_COLUMN = 'sentiment'\n",
    "LABEL_VALUES = [-1, 0, 1]\n",
    "CLASS_NAMES = ['negative', 'neutral', 'positive']\n",
    "\n",
    "LABEL_MAP = {}\n",
    "for (i, label) in enumerate(LABEL_VALUES):\n",
    "    LABEL_MAP[label] = i\n",
    "    \n",
    "    \n",
    "def parse_args():\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    ###### CLI args\n",
    "    parser.add_argument('--train_batch_size', \n",
    "                        type=int, \n",
    "                        default=128, metavar='N',\n",
    "                        help='input batch size for training (default: 128)')\n",
    "\n",
    "    parser.add_argument('--validation_batch_size', \n",
    "                        type=int, \n",
    "                        default=128, metavar='N',\n",
    "                        help='input batch size for validation (default: 128)') \n",
    "    \n",
    "    parser.add_argument('--test_batch_size', \n",
    "                        type=int, \n",
    "                        default=128, metavar='N',\n",
    "                        help='input batch size for testing (default: 128)')\n",
    "    \n",
    "    parser.add_argument('--epochs', \n",
    "                        type=int, \n",
    "                        default=10, metavar='N',\n",
    "                        help='number of epochs to train (default: 10)')\n",
    "    \n",
    "    parser.add_argument('--lr', \n",
    "                        type=float, \n",
    "                        default=0.01, metavar='LR',\n",
    "                        help='learning rate (default: 0.01)')\n",
    "    \n",
    "    parser.add_argument('--momentum', \n",
    "                        type=float, \n",
    "                        default=0.5, metavar='M',\n",
    "                        help='SGD momentum (default: 0.5)')\n",
    "    \n",
    "    parser.add_argument('--seed', \n",
    "                        type=int, \n",
    "                        default=42, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    \n",
    "    parser.add_argument('--log_interval', \n",
    "                        type=int, \n",
    "                        default=100, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    \n",
    "    parser.add_argument('--backend', \n",
    "                        type=str, \n",
    "                        default=None,\n",
    "                        help='backend for distributed training (tcp, gloo on cpu and gloo, nccl on gpu)')\n",
    "    \n",
    "    parser.add_argument('--max_seq_len', \n",
    "                        type=int, \n",
    "                        default=64, \n",
    "                        help='max sequence length of input tokens')\n",
    "    \n",
    "    parser.add_argument(\"--model_name\", \n",
    "                        type=str, \n",
    "                        default=MODEL_NAME, \n",
    "                       help='Model name')\n",
    "    \n",
    "    parser.add_argument('--enable_sagemaker_debugger', \n",
    "                        type=eval, \n",
    "                        default=False)\n",
    "    \n",
    "    parser.add_argument('--run_validation', \n",
    "                        type=eval,\n",
    "                        default=False)  \n",
    "    \n",
    "    parser.add_argument('--run_test', \n",
    "                        type=eval, \n",
    "                        default=False)    \n",
    "    \n",
    "    parser.add_argument('--run_sample_predictions', \n",
    "                        type=eval, \n",
    "                        default=False)\n",
    "    \n",
    "    parser.add_argument('--train_steps_per_epoch',\n",
    "                        type=int,\n",
    "                        default=None)\n",
    "\n",
    "    ###### Container environment   \n",
    "    parser.add_argument('--hosts', \n",
    "                        type=list, \n",
    "                        default=json.loads(os.environ['SM_HOSTS']))\n",
    "    \n",
    "    parser.add_argument('--current_host', \n",
    "                        type=str, \n",
    "                        default=os.environ['SM_CURRENT_HOST'])\n",
    "    \n",
    "    parser.add_argument('--model_dir', \n",
    "                        type=str, \n",
    "                        default=os.environ['SM_MODEL_DIR'])\n",
    "\n",
    "    parser.add_argument('--train_data', \n",
    "                        type=str, \n",
    "                        default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "    \n",
    "    parser.add_argument('--validation_data', \n",
    "                        type=str, \n",
    "                        default=os.environ['SM_CHANNEL_VALIDATION'])\n",
    "    \n",
    "    parser.add_argument('--test_data', \n",
    "                        type=str, \n",
    "                        default=os.environ['SM_CHANNEL_TEST'])\n",
    "    \n",
    "    parser.add_argument('--output_dir', \n",
    "                        type=str, \n",
    "                        default=os.environ['SM_OUTPUT_DIR'])\n",
    "    \n",
    "    parser.add_argument('--num_gpus', \n",
    "                        type=int, \n",
    "                        default=os.environ['SM_NUM_GPUS'])\n",
    "    \n",
    "    # Debugger Args\n",
    "    parser.add_argument(\"--save-frequency\", \n",
    "                        type=int, \n",
    "                        default=10, \n",
    "                        help=\"frequency with which to save steps\")\n",
    "    \n",
    "    parser.add_argument(\"--smdebug_path\",\n",
    "                        type=str,\n",
    "                        help=\"output directory to save data in\",\n",
    "                        default=\"/opt/ml/output/tensors\",)\n",
    "    \n",
    "    parser.add_argument(\"--hook-type\",\n",
    "                        type=str,\n",
    "                        choices=[\"saveall\", \"module-input-output\", \"weights-bias-gradients\"],\n",
    "                        default=\"saveall\",)\n",
    "\n",
    "    print(sys.argv)\n",
    "    return parser.parse_args(sys.argv[1:])\n",
    "\n",
    "\n",
    "\n",
    "class ReviewDataset(Dataset):\n",
    "\n",
    "    def __init__(self, reviews, targets, tokenizer, max_seq_len):\n",
    "        self.reviews = reviews\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        review = str(self.reviews[item])\n",
    "        target = self.targets[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "          review,\n",
    "          add_special_tokens=True,\n",
    "          max_length=self.max_seq_len,\n",
    "          return_token_type_ids=False,\n",
    "          padding='max_length',\n",
    "          return_attention_mask=True,\n",
    "          return_tensors='pt',\n",
    "          truncation=True\n",
    "        )\n",
    "        \n",
    "        tokens = self.tokenizer.tokenize(review)[0:self.max_seq_len] #, truncation=True, max_length=self.max_seq_len)\n",
    "        tokens += [''] * (self.max_seq_len - len(tokens))\n",
    "\n",
    "        # print('type(tokens): {}'.format(type(tokens)))        \n",
    "        # print('type(encoding.input_ids): {}'.format(type(encoding['input_ids'].flatten())))\n",
    "        # print('encoding.input_ids: {}'.format(encoding['input_ids'].flatten()))\n",
    "        print('************** START **************************')\n",
    "        print('tokens: {}'.format(tokens))\n",
    "        print('encoding.input_ids: {}'.format(encoding['input_ids'].flatten()))\n",
    "        print('encoding.input_ids.flatten().tolist()[1:]: {}'.format(encoding['input_ids'].flatten().tolist()[1:]))\n",
    "\n",
    "        print('list len encoding.input_ids.flatten(): {}'.format(len(encoding['input_ids'].flatten())))\n",
    "        print('list len encoding.input_ids.flatten().tolist()[1:]: {}'.format(len(encoding['input_ids'].flatten().tolist()[1:])))\n",
    "        print('list len tokens: {}'.format(len(tokens)))\n",
    "        print('encoding.input_ids.flatten().tolist()[1:]: {}'.format(encoding['input_ids'].flatten().tolist()[1:]))\n",
    "        \n",
    "        tokens_to_input_ids = zip(tokens, encoding['input_ids'].flatten().tolist()[1:])\n",
    "        print('tokens_to_input_ids: {}'.format(tokens_to_input_ids))\n",
    "        for token, input_id in zip(tokens, encoding['input_ids'].flatten().tolist()[1:]):\n",
    "              print('token: {}'.format(token))\n",
    "              print('input_id: {}'.format(input_id))\n",
    "        print('**************** END ************************')\n",
    "        \n",
    "        # TODO: CHECK IF TOKENS IS GOOD ENOUGH SIMILAR TO NATHALIE'S EXAMPLE   \n",
    "        return encoding['input_ids'].flatten(), torch.tensor(target, dtype=torch.long), tokens\n",
    "\n",
    "    \n",
    "def create_list_input_files(path):\n",
    "    input_files = glob.glob('{}/*.tsv'.format(path))\n",
    "    print(input_files)\n",
    "    return input_files\n",
    "\n",
    "    \n",
    "def create_data_loader(path, tokenizer, max_seq_len, batch_size):\n",
    "    logger.info(\"Get data loader\")\n",
    "\n",
    "    df = pd.DataFrame(columns=['sentiment', 'review_body'])\n",
    "    \n",
    "    input_files = create_list_input_files(path)\n",
    "\n",
    "    for file in input_files:\n",
    "        df_temp = pd.read_csv(file, \n",
    "                              sep='\\t', \n",
    "                              usecols=['sentiment', 'review_body']\n",
    "                             )\n",
    "        df = df.append(df_temp)\n",
    "        \n",
    "    print(len(df))\n",
    "    print('df[sentiment]: {}'.format(df['sentiment']))\n",
    "    \n",
    "    df['sentiment'] = df.sentiment.apply(lambda sentiment: LABEL_MAP[sentiment])\n",
    "    print('df[sentiment] after LABEL_MAP: {}'.format(df['sentiment']))\n",
    "    print(df.head())\n",
    "    \n",
    "    ds = ReviewDataset(\n",
    "        reviews=df.review_body.to_numpy(),\n",
    "        targets=df.sentiment.to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_len=max_seq_len\n",
    "    )\n",
    "    \n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    ), df\n",
    "\n",
    "\n",
    "\n",
    "# TODO: need to put saved config.json in code/ folder\n",
    "def save_transformer_model(model, model_dir):\n",
    "    path = '{}/transformer'.format(model_dir)\n",
    "    os.makedirs(path, exist_ok=True)                              \n",
    "    logger.info('Saving Transformer model to {}'.format(path))\n",
    "    model.save_pretrained(path)\n",
    "\n",
    "\n",
    "# Needs to saved in model_dir root folder\n",
    "def save_pytorch_model(model, model_dir):\n",
    "    # path = '{}/pytorch'.format(model_dir)\n",
    "    os.makedirs(model_dir, exist_ok=True) \n",
    "    logger.info('Saving PyTorch model to {}'.format(model_dir))\n",
    "    save_path = os.path.join(model_dir, MODEL_NAME)\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "def load_transformer_model(model_dir):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    config = RobertaConfig.from_json_file('{}/config.json'.format(model_dir))\n",
    "    model = RobertaForSequenceClassification.from_pretrained(model_dir, config=config)\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "def load_pytorch_model(model_dir):\n",
    "    model_path = '{}/{}'.format(model_dir, MODEL_NAME)\n",
    "    model = RobertaForSequenceClassification()\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        model.load_state_dict(torch.load(model_path, map_location='cuda:0'))  \n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))    \n",
    "    return model\n",
    "    \n",
    "\n",
    "def train_model(model,\n",
    "                train_data_loader,\n",
    "                df_train,\n",
    "                val_data_loader, \n",
    "                df_val,\n",
    "                args):\n",
    "    \n",
    "#    hook = smd.Hook.create_from_json_file()   \n",
    "    \n",
    "    loss_function = nn.CrossEntropyLoss()    \n",
    "    optimizer = optim.Adam(params=model.parameters(), lr=args.lr)\n",
    "    \n",
    "    if args.enable_sagemaker_debugger:\n",
    "        print('Enable SageMaker Debugger.')\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        print('EPOCH -- {}'.format(epoch))\n",
    "\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for i, (sent, label, tokens) in enumerate(train_data_loader):\n",
    "#            hook.set_mode(modes.TRAIN)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            sent = sent.squeeze(0)\n",
    "            print('tokens: {}'.format(tokens))\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                sent = sent.cuda()\n",
    "                label = label.cuda()\n",
    "            output = model.forward(sent)[0]\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            \n",
    "            loss = loss_function(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if i%100 == 0:\n",
    "                train_total += label.size(0)\n",
    "                train_correct += (predicted.cpu() == label.cpu()).sum()\n",
    "                accuracy = 100.00 * train_correct.numpy() / train_total\n",
    "                print('[epoch: {0} / step: {1}] train_loss: {2:.2f} - train_acc: {3:.2f}%'.format(epoch, i, loss.item(), accuracy))\n",
    "                        \n",
    "            if args.run_validation:\n",
    "                # hook.set_mode(modes.EVAL)\n",
    "\n",
    "                if i%100 == 0:\n",
    "                    print('RUNNING VALIDATION:')\n",
    "                    correct = 0\n",
    "                    total = 0\n",
    "                    model.eval()\n",
    "                    for sent, label, tokens in val_data_loader:\n",
    "                        sent = sent.squeeze(0)\n",
    "                        if torch.cuda.is_available():\n",
    "                            sent = sent.cuda()\n",
    "                            label = label.cuda()\n",
    "                        output = model.forward(sent)[0]\n",
    "                        _, predicted = torch.max(output.data, 1)\n",
    "                        \n",
    "                        total += label.size(0)\n",
    "                        correct += (predicted.cpu() == label.cpu()).sum()\n",
    "                        \n",
    "                        print('tokens: {}'.format(tokens))\n",
    "                \n",
    "                    accuracy = 100.00 * correct.numpy() / total\n",
    "                    print('[epoch: {0} / step: {1}] val_loss: {2:.2f} - val_acc: {3:.2f}%'.format(epoch, i, loss.item(), accuracy))\n",
    "\n",
    "    print('TRAINING COMPLETED.')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pprint\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "from transformers import RobertaModel, RobertaTokenizer, RobertaConfig\n",
    "from transformers import RobertaForSequenceClassification\n",
    "\n",
    "# from src.utils_simple import create_data_loader, train_model, parse_args, save_pytorch_model, save_transformer_model\n",
    "#import create_data_loader, train_model, parse_args, save_pytorch_model, save_transformer_model\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "# Has to be called 'model.pth'\n",
    "MODEL_NAME = 'model.pth'\n",
    "PRE_TRAINED_MODEL_NAME = 'roberta-base'\n",
    "\n",
    "DATA_COLUMN = 'review_body'\n",
    "LABEL_COLUMN = 'sentiment'\n",
    "LABEL_VALUES = [-1, 0, 1]\n",
    "CLASS_NAMES = ['negative', 'neutral', 'positive']\n",
    "\n",
    "LABEL_MAP = {}\n",
    "for (i, label) in enumerate(LABEL_VALUES):\n",
    "    LABEL_MAP[label] = i\n",
    "\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "\n",
    "###### Parse ARGS\n",
    "# args = parse_args()\n",
    "# print('Loaded arguments:')\n",
    "# print(args)\n",
    "\n",
    "\n",
    "###### Get Environment Variables\n",
    "env_var = os.environ \n",
    "print('Environment variables:')\n",
    "pprint.pprint(dict(env_var), width = 1) \n",
    "\n",
    "#    print('SM_TRAINING_ENV {}'.format(env_var['SM_TRAINING_ENV']))\n",
    "#    sm_training_env_json = json.loads(env_var['SM_TRAINING_ENV'])\n",
    "\n",
    "###### Check if Training Master\n",
    "#    is_master = sm_training_env_json['is_master']\n",
    "#    print('is_master {}'.format(is_master))\n",
    "\n",
    "#     if is_master:\n",
    "#         checkpoint_path = args.checkpoint_base_path\n",
    "#     else:\n",
    "#         checkpoint_path = '/tmp/checkpoints'        \n",
    "#     print('checkpoint_path {}'.format(checkpoint_path))\n",
    "\n",
    "###### Check if distributed training\n",
    "# is_distributed = len(args.hosts) > 1 and args.backend is not None\n",
    "is_distributed = False\n",
    "\n",
    "logger.debug(\"Distributed training - {}\".format(is_distributed))\n",
    "use_cuda = args.num_gpus > 0\n",
    "logger.debug(\"Number of gpus available - {}\".format(args.num_gpus))\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "if is_distributed:\n",
    "    ###### Initialize the distributed environment.\n",
    "    world_size = len(args.hosts)\n",
    "    os.environ['WORLD_SIZE'] = str(world_size)\n",
    "    host_rank = args.hosts.index(args.current_host)\n",
    "    os.environ['RANK'] = str(host_rank)\n",
    "    dist.init_process_group(backend=args.backend, rank=host_rank, world_size=world_size)\n",
    "    logger.info('Initialized the distributed environment: \\'{}\\' backend on {} nodes. '.format(\n",
    "        args.backend, dist.get_world_size()) + 'Current host rank is {}. Number of gpus: {}'.format(\n",
    "        dist.get_rank(), args.num_gpus))\n",
    "\n",
    "###### Set the seed for generating random numbers\n",
    "torch.manual_seed(args.seed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(args.seed) \n",
    "\n",
    "\n",
    "###### INSTANTIATE MODEL\n",
    "tokenizer = None\n",
    "config = None\n",
    "model = None\n",
    "\n",
    "successful_download = False\n",
    "retries = 0\n",
    "\n",
    "while (retries < 5 and not successful_download):\n",
    "    try:\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "\n",
    "        config = RobertaConfig.from_pretrained(PRE_TRAINED_MODEL_NAME,\n",
    "                                               num_labels=len(CLASS_NAMES),\n",
    "                                               id2label={\n",
    "                                                   0: -1,\n",
    "                                                   1: 0,\n",
    "                                                   2: 1,\n",
    "                                               },\n",
    "                                               label2id={\n",
    "                                                   -1: 0,\n",
    "                                                   0: 1,\n",
    "                                                   1: 2,\n",
    "                                               })\n",
    "        config.output_attentions=True\n",
    "        model = RobertaForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL_NAME, \n",
    "                                                                 config=config)\n",
    "        model.to(device)\n",
    "        successful_download = True\n",
    "        print('Sucessfully downloaded after {} retries.'.format(retries))\n",
    "\n",
    "    except:\n",
    "        retries = retries + 1\n",
    "        random_sleep = random.randint(1, 30)\n",
    "        print('Retry #{}.  Sleeping for {} seconds'.format(retries, random_sleep))\n",
    "        time.sleep(random_sleep)\n",
    "\n",
    "if not tokenizer or not model or not config:\n",
    "     print('Not properly initialized...')\n",
    "\n",
    "###### CREATE DATA LOADERS\n",
    "train_data_loader, df_train = create_data_loader(args.train_data, tokenizer, args.max_seq_len, args.train_batch_size)\n",
    "val_data_loader, df_val = create_data_loader(args.validation_data, tokenizer, args.max_seq_len, args.validation_batch_size)\n",
    "\n",
    "logger.debug(\"Processes {}/{} ({:.0f}%) of train data\".format(\n",
    "    len(train_data_loader.sampler), len(train_data_loader.dataset),\n",
    "    100. * len(train_data_loader.sampler) / len(train_data_loader.dataset)\n",
    "))\n",
    "\n",
    "logger.debug(\"Processes {}/{} ({:.0f}%) of test data\".format(\n",
    "    len(val_data_loader.sampler), len(val_data_loader.dataset),\n",
    "    100. * len(val_data_loader.sampler) / len(val_data_loader.dataset)\n",
    ")) \n",
    "\n",
    "# model_dir = os.environ['SM_MODEL_DIR']\n",
    "print('model_dir: {}'.format(args.model_dir))\n",
    "\n",
    "print('model summary: {}'.format(model))\n",
    "\n",
    "###### START TRAINING\n",
    "\n",
    "model = train_model(model,\n",
    "                    train_data_loader,\n",
    "                    df_train,\n",
    "                    val_data_loader, \n",
    "                    df_val,\n",
    "                    args)\n",
    "\n",
    "save_transformer_model(model, args.model_dir)\n",
    "save_pytorch_model(model, args.model_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
